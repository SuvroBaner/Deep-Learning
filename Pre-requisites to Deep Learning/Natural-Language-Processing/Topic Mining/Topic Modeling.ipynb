{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, unicode_literals\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "###\n",
    "\n",
    "def clean_init_data(df, speech):\n",
    "    if speech:\n",
    "        df['mtn'] = df['mtn'].apply(str)\n",
    "        df['script'] = df['script'].apply(str)\n",
    "        #df[df.columns[1]] = df[df.columns[1]].apply(lambda r: pd.to_datetime(r, errors='coerce'))\n",
    "        #df['mtn'] = df['mtn'].apply(str)\n",
    "        #df['start_tm'] = df['start_tm'].apply(lambda r: pd.to_datetime(r, errors='coerce'))\n",
    "        #df['script'] = df['script'].apply(str)\n",
    "    elif speech == 'Adhoc':\n",
    "        df['mtn'] = df['mtn'].apply(str)\n",
    "        df['script'] = df['script'].apply(str)\n",
    "    else:\n",
    "        df = df.drop_duplicates(subset='april_chat_campaigndetails.engagement_id')\n",
    "        df = df[['april_chat_campaigndetails.engagement_id', 'april_chat_campaigndetails.customer_chat_text']]\n",
    "        df['april_chat_campaigndetails.customer_chat_text'] = df['april_chat_campaigndetails.customer_chat_text'].apply(str)\n",
    "        df['april_chat_campaigndetails.engagement_id'] = df['april_chat_campaigndetails.engagement_id'].apply(str)\n",
    "    return df\n",
    "\n",
    "def min_date(val):\n",
    "    return val.min()\n",
    "\n",
    "def concat_text(val):\n",
    "    return ' '.join(val)\n",
    "\n",
    "def get_data(speech):\n",
    "    if speech == 'Speech':\n",
    "        FILEPATH = '/apps/opt/applicaitons/datasets/Datasets/raw_transcript_5000.txt'\n",
    "        SCRIPT = 'script'\n",
    "        df_orig = pd.read_table(FILEPATH, sep = '\\t')\n",
    "        df_orig.rename(columns={df_orig.columns[0]: 'mtn',\n",
    "                        df_orig.columns[1]: 'start_tm',\n",
    "                        df_orig.columns[4]: 'script'}, inplace=True)\n",
    "        df = df_orig\n",
    "        df = clean_init_data(df, speech)\n",
    "        df = df.groupby(['mtn']).agg({'start_tm': min_date, 'script': concat_text}).reset_index()\n",
    "        \n",
    "    elif speech == 'Adhoc':\n",
    "        FILEPATH = '/apps/opt/applicaitons/datasets/Datasets/chat_adhoc.txt'\n",
    "        SCRIPT = 'script'\n",
    "        df_orig = pd.read_table(FILEPATH, sep = '\\t')\n",
    "        df_orig.rename(columns={df_orig.columns[0]: 'mtn',\n",
    "                        df_orig.columns[1]: 'script'}, inplace=True)\n",
    "        df = df_orig\n",
    "        df = clean_init_data(df, speech)\n",
    "    else:\n",
    "        FILEPATH = '/apps/opt/applicaitons/datasets/Datasets/campaigndetails.tsv'\n",
    "        SCRIPT = 'april_chat_campaigndetails.customer_chat_text'\n",
    "        df_orig = pd.read_table(FILEPATH, sep = '\\t')\n",
    "        df = df_orig\n",
    "        df_orig = df_orig.drop_duplicates(subset='april_chat_campaigndetails.engagement_id')\n",
    "        df = clean_init_data(df, speech)\n",
    "        \n",
    "    return df_orig, df, SCRIPT\n",
    "\n",
    "df_orig, final_df, SCRIPT = get_data(speech = 'Speech')\n",
    "\n",
    "### Data Cleaning and Tokenization\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "bogus_words = ['hi', 'ok', 'okay', 'yes', 'customer', 'hello', 'hmm', 'hmmm', 'hmmmm', 'thnx', 'ty', 'ohk', 'hey', \n",
    "               'tq', 'hmmk', 'blah', 'bla', 'tku', 'thank', 'thanks', 'nt', 'u', 'e', 'en', 'xx', 't', 'yeah', 'youre', \n",
    "               'yeah', 'youre', 'thats', 'thing', 'dont', 'something', 'maam', 'everything', 'name', 'dont', 'anything',\n",
    "              'didnt', 'mean', 'look', 'guy', 'kind', 'moment', 'wife', 'yesterday', 'sorry', 'not', 'can', 'right',\n",
    "              'gosh', 'today', 'amir', 'cent', 'forty', 'please', 'system', 'person', 'people', 'can', 'not',\n",
    "              'doe', 'number', 'hi', 'ok', 'okay', 'yes', 'customer', 'hello', 'hmm', 'hmmm', 'hmmmm', 'thnx', 'ty', 'ohk',\n",
    "             'hey', 'tq', 'hmmk', 'blah', 'bla', 'tku', 'none', 'None', 'haha', 'gt', 'etc', 'hehe', 'se',\n",
    "            'yo', 'ya', 'omg', 'technically', 'gracias', 'ah', 'un', 'ci', 'certainly', 'ki', 'dot', 'ti',\n",
    "            'etc', 'regularly', 'outright', 'bc', 'truly', 'nearly', 'txt', 'significantly', 'per', 'yep',\n",
    "            'yup', 'nd', 'immediately', 'ma', 'tho', 'umm', 'np', 'gotcha', 'zy', 'lately', 'extremely',\n",
    "            'ho', 'enjoy', 'youre', 'youve', 'yea', 'yeah', 'heck', 'thru', 'ha', 'sec', 'yup', 'na',\n",
    "            'thanks', 'thank', 'nt', 'en', 'xx', 'dont', 'thats', 'something', 'maam', 'everything',\n",
    "            'didnt', 'mean', 'look', 'guy', 'kind', 'moment', 'yesterday', 'sorry', 'not', 'can', 'right',\n",
    "            'gosh', 'today', 'amir', 'cent', 'please', 'person', 'people', 'can', 'not', 'doe', 'oh',\n",
    "            'got', 'go', 'like', 'guess', 'much', 'far', 'im', 'still', 'actually', 'really', 'awesome',\n",
    "            'said', 'hope', 'sure', 'sorry', 'because', 'let', 'morning', 'curious', 'bit', 'cool', 'ti',\n",
    "            'well', 'nope', 'also', 'basically', 'th', 'null', 'NULL', 'Null', 'its', 'nothing', 'pa', 'mike', 'died',\n",
    "              'parent', 'isnt', 'tomorrow', 'husband', 'yet', 'want', 'town', 'family', 'years', 'them', 'meet', 'ampamp',\n",
    "              'stephen', 'till', 'daughter', 'done', 'get', 'you']\n",
    "\n",
    "def fn_tokenizer(s):\n",
    "    s = re.sub('x{2,}', '', s)  # remove masking\n",
    "    s = re.sub('[*]', '', s) # remove masking\n",
    "    s = re.sub('\\d', '', s) # remove digits\n",
    "    #s = s.translate(None, string.punctuation) # remove punctuations... some version issue\n",
    "    s = re.sub(' +', ' ', s) # remove multiple spaces\n",
    "    s = s.lower() # making everyhing lower case\n",
    "    s = ' '.join(word for word in s.split(' ') if len(word) > 3)\n",
    "    s = ' '.join(wordnet_lemmatizer.lemmatize(word) for word in s.split(' '))\n",
    "    s = ' '.join(lemmatizer.lemmatize(word) for word in s.split(' '))\n",
    "    pos_tuple = pos_tag(s.split()) # parts of speech tagging\n",
    "    s = ' '.join(word for word, pos in pos_tuple if pos in ['NN', 'NNP', 'NNS', 'NNPS']) # returning only noun\n",
    "    s = re.sub('''[,.!?:;\\\"']''', '', s) # # remove punctuations\n",
    "    s = ' '.join(word for word in s.split(' ') if word not in bogus_words) # removing bogus words\n",
    "    #tokens = nltk.tokenize.word_tokenize(s) # returns the tokens\n",
    "    #tokens = [t for t in tokens if len(t) > 3] # only picks the word whose length is more than 3\n",
    "    #tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]  # lemmatize the tokens\n",
    "    return s\n",
    "\n",
    "final_df[SCRIPT] = final_df[SCRIPT].apply(fn_tokenizer)\n",
    "final_df = final_df[final_df[SCRIPT].apply(lambda x: len(x) > 0)]  # removing the records which has no values.\n",
    "final_df = final_df.reset_index()\n",
    "del final_df['index']\n",
    "\n",
    "### TF-IDF and Topic Modeling\n",
    "\n",
    "# Term Frequency : Number of times word appears in a document blob, \n",
    "# normalized by dividing by the total number of words in the blob\n",
    "# We use TextBlob for breaking up the text into words and getting the word counts.\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "# Returns the number of documents containing word.\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "# Computes inverse document frequency, which measures how common a word is among all documents in the bloblist.\n",
    "# The more common the word is, lower the idf. We take the ratio of the total number of documents to the number of \n",
    "# documents containing word, then take a log of that. Add 1 to the divisor to prevent division by 0.\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "# It computes the TF-IDF score\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "# Use only if you are using TF:\n",
    "\n",
    "def fn_topWords(df):\n",
    "    bloblist = []\n",
    "    for i, j in df[SCRIPT].iteritems():\n",
    "        bloblist.append(tb(j))\n",
    "\n",
    "    pre_topic_dict = {}\n",
    "    for i, blob in enumerate(bloblist):\n",
    "        top_words = []\n",
    "        high_info_words = []\n",
    "        #scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "        scores = {word: tf(word, blob) for word in blob.words}\n",
    "        sorted_words = sorted(scores.items(), key = lambda x: x[1], reverse = True)\n",
    "        threshold = np.nanpercentile([pair[1] for pair in sorted_words], 25) #  taking above 25 percentile\n",
    "        top_words = [pair[0] for pair in sorted_words if pair[1] > threshold]  # get all the words for that doc\n",
    "        for ind, word in enumerate(sorted(blob.words)):\n",
    "            if word in sorted(top_words):\n",
    "                high_info_words.append(word)\n",
    "        pre_topic_dict[i] =  high_info_words\n",
    "    return pre_topic_dict\n",
    "\n",
    "pre_topic_dict = fn_topWords(final_df)\n",
    "\n",
    "def pickWords(x, dct):\n",
    "    thresh = np.nanpercentile([v for k, v in dct.items() if k in x], 25)\n",
    "    high_info_words = [a for a, b in [(k, v) for k, v in dct.items() if k in x] if b >= thresh]\n",
    "    \n",
    "    high_info_list = []\n",
    "    for word in x.split(' '):\n",
    "        if word in high_info_words:\n",
    "            high_info_list.append(word)\n",
    "    return high_info_list\n",
    "\n",
    "def computeTF_IDF(df):\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    corpus = df[SCRIPT]\n",
    "    vectorizer.fit_transform(corpus)\n",
    "    idf = vectorizer.idf_\n",
    "    a_dict = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "    #thresh = df[SCRIPT].apply(lambda x : calcThreshold(x, a_dict))\n",
    "    #thresh = np.nanpercentile([v for k, v in a_dict.items() if k in final_df[SCRIPT][5]], 25)\n",
    "    #thresh = np.percentile(idf, 25)\n",
    "    df[SCRIPT] = df[SCRIPT].apply(lambda x: pickWords(x, a_dict))\n",
    "    df = df[df[SCRIPT].apply(lambda x: len(x) > 0)]  # removing the records which has no values.\n",
    "    df = df.reset_index()\n",
    "    del df['index']\n",
    "\n",
    "    pre_topic_dict = {}\n",
    "    for ind, val in enumerate(df[SCRIPT]):\n",
    "        pre_topic_dict[ind] = val\n",
    "    \n",
    "    return pre_topic_dict, df\n",
    "\n",
    "#pre_topic_dict, final_df = computeTF_IDF(final_df)\n",
    "\n",
    "def optimize_topic(lsamodel, doc_term_matrix):\n",
    "    Lsi_2d_data = []  # output of svd will be stored here\n",
    "\n",
    "    #for v in lsamodel[doc_term_matrix]:\n",
    "    #    print(v)\n",
    "    \n",
    "    for vector in lsamodel[doc_term_matrix]:\n",
    "        if len(vector) != 2:\n",
    "            continue\n",
    "        Lsi_2d_data.append((vector[0][1], vector[1][1]))\n",
    "        \n",
    "    # Next I clustered the points in the reduced 2D LSI space using KMeans, varying the number of clusters (K) from 1 to 10\n",
    "    # The objective function used is the Inertia of the cluster, defined as the sum of squared differences of each point \n",
    "    # to its cluster centroid\n",
    "    \n",
    "    MAX_K = 10\n",
    "\n",
    "    X = Lsi_2d_data\n",
    "    ks = range(1, MAX_K + 1)\n",
    "\n",
    "    inertias = np.zeros(MAX_K)\n",
    "    diff = np.zeros(MAX_K)\n",
    "    diff2 = np.zeros(MAX_K)\n",
    "    diff3 = np.zeros(MAX_K)\n",
    "\n",
    "    for k in ks:\n",
    "        kmeans = KMeans(k).fit(X)\n",
    "        inertias[k - 1] = kmeans.inertia_\n",
    "        # first difference    \n",
    "        if k > 1:\n",
    "            diff[k - 1] = inertias[k - 1] - inertias[k - 2]\n",
    "        # second difference\n",
    "        if k > 2:\n",
    "            diff2[k - 1] = diff[k - 1] - diff[k - 2]\n",
    "        # third difference\n",
    "        if k > 3:\n",
    "            diff3[k - 1] = diff2[k - 1] - diff2[k - 2]\n",
    "\n",
    "    elbow = np.argmin(diff3[3:]) + 3\n",
    "    \n",
    "    num_of_topics = ks[elbow]\n",
    "    \n",
    "    plt.plot(ks, inertias, \"b*-\")\n",
    "    plt.plot(ks[elbow], inertias[elbow], marker='o', markersize=12,\n",
    "             markeredgewidth=2, markeredgecolor='r', markerfacecolor=None)\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The Number of Optimal Topics: \", num_of_topics)\n",
    "    \n",
    "    #X = a\n",
    "    #kmeans = KMeans(NUM_TOPICS).fit(X)\n",
    "    #y = kmeans.labels_\n",
    "\n",
    "    #colors = [\"b\", \"g\", \"r\", \"m\", \"c\"]\n",
    "    #for i in range(np.array(X).shape[0]):\n",
    "    #    plt.scatter(X[i][0], X[i][1], c=colors[y[i]], s=10)    \n",
    "    #plt.show()\n",
    "    \n",
    "    return num_of_topics\n",
    "\n",
    "def topic_modeling(pre_topic_dict):\n",
    "    doc_clean = []\n",
    "    for k, v in pre_topic_dict.items():\n",
    "        doc_clean.append(v)\n",
    "        \n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    \n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    \n",
    "    # I didn't know, how many topics this corpus should yield, so I decided to compute this by reducing the features to \n",
    "    # two dimensions, then clustering the points for different values of K (number of clusters) to find an optimum value.\n",
    "    # To project the vectors in a corpus to a different coordinate space (say 2D), we will use LSA (Latent Semantic Analysis).\n",
    "    # This is Singular Value Decomposition (SVD)\n",
    "    \n",
    "    Lsi = gensim.models.LsiModel\n",
    "    \n",
    "    # project to 2 dimensions for visualization\n",
    "    lsamodel = Lsi(doc_term_matrix, id2word=dictionary, num_topics=2)\n",
    "    \n",
    "    num_of_topics = optimize_topic(lsamodel, doc_term_matrix)\n",
    "    #num_of_topics = 15\n",
    "    \n",
    "    # Running LDA Model\n",
    "    # Next step is to create an object for LDA model and train it on Document-Term matrix. \n",
    "    # Creating the object for LDA model using gensim library\n",
    "    #Lda = gensim.models.ldamodel.LdaModel\n",
    "    Lda = gensim.models.ldamulticore.LdaMulticore\n",
    "    \n",
    "    # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=num_of_topics, id2word = dictionary, passes=1, chunksize = 500)\n",
    "    \n",
    "    return ldamodel, doc_term_matrix, num_of_topics\n",
    "\n",
    "lda_model, dtm, num_of_topics = topic_modeling(pre_topic_dict)\n",
    "\n",
    "def find_topic(aList):\n",
    "    num_topics = len(aList)\n",
    "    mean_top_prob = np.mean([lst_itm[1] for lst_itm in aList]) - .01  # to take the border line topics (1 % tolerance)\n",
    "    if num_topics == 1:\n",
    "        return [aList[0][0]]  # return the only topic\n",
    "    elif num_topics == 2:\n",
    "        return [lst_itm[0] for lst_itm in aList if lst_itm[1] >= mean_top_prob + .05] # one topic should have at least 55% probability\n",
    "    else:\n",
    "        return [lst_itm[0] for lst_itm in aList if lst_itm[1] >= mean_top_prob*1.2] # one topic atleast 1.2 the mean prob.\n",
    "    \n",
    "\n",
    "def topic_doc_map(lda_model, dtm, num_of_topics):\n",
    "    lda_corpus = lda_model[dtm]\n",
    "    \n",
    "    topic_doc_dict = {}\n",
    "    for doc, topic in enumerate(lda_corpus):\n",
    "        #topic_doc_dict[doc] = sorted(topic, key = lambda x: x[1], reverse = True)[0][0]\n",
    "        topic_doc_dict[doc] = find_topic(topic)\n",
    "        \n",
    "    topic_word_dict = {}\n",
    "    num_of_topics = num_of_topics\n",
    "\n",
    "    all_topics = []\n",
    "    for topic in range(num_of_topics):\n",
    "        all_tokens = lda_model.get_topic_terms(topic) # it's a tuple (word_id, weight) for a given topic\n",
    "        wt_thresh = np.percentile([tup[1] for tup in all_tokens], 25) # setting a weight threshold\n",
    "        selected_tokens = [tup[0] for tup in all_tokens if tup[1] > wt_thresh]\n",
    "        total_tokens = len(selected_tokens)\n",
    "        all_topics.append((topic, lda_model.print_topic(topic, total_tokens))) # final topic and number of tokens list\n",
    "        \n",
    "    all_topics = [(tup[0], (lambda a: re.findall(\"[a-zA-Z]+\", a))(tup[1])) for tup in all_topics]  # removing the weights\n",
    "    \n",
    "    for topic in all_topics:\n",
    "        topic_word_dict[topic[0]] = topic[1]\n",
    "        \n",
    "    return topic_doc_dict, topic_word_dict\n",
    "\n",
    "topic_doc_dict, topic_word_dict = topic_doc_map(lda_model, dtm, num_of_topics)\n",
    "\n",
    "def topic_extract(r):\n",
    "    num_top = len(r)\n",
    "    if num_top == 0:\n",
    "        return 'OTHER_TOPIC'\n",
    "    elif num_top == 1:\n",
    "        return topic_word_dict[r[0]]\n",
    "    elif num_top > 1:\n",
    "        return dict((el, topic_word_dict[el]) for el in r)\n",
    "    \n",
    "\n",
    "def present_data(final_df, topic_doc_dict, topic_word_dict):\n",
    "    topic_df = pd.concat((final_df, pd.Series(topic_doc_dict, name='topic')), axis = 1)\n",
    "    topic_df['topic_tokens'] = topic_df['topic'].apply(topic_extract)\n",
    "    topic_df.to_csv('final_output_text_topic.csv')\n",
    "    return topic_df\n",
    "\n",
    "topic_word_dict\n",
    "\n",
    "present_data(final_df, topic_doc_dict, topic_word_dict).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
