{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################  Importing the required packages  ########################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import operator\n",
    "import sys\n",
    "\n",
    "#################################################### Utility Functions  ####################################################\n",
    "\n",
    "def get_data(filepath):\n",
    "    #df = pd.read_csv(filepath)\n",
    "    df = pd.read_table(filepath, sep = '\\t', dtype=object)\n",
    "    df.fillna('Missing', inplace = True)\n",
    "    df = df.ix[:, [0, 2, 3, 7]] # selecting the relevant column from the flat file ### HARDCODED ###\n",
    "    df.columns = ['Session_ID', 'Timestamp', 'Links', \"Pages\"]\n",
    "    df['Pages'] = df['Pages'].apply(str)\n",
    "    df['Pages'] = df['Pages'].apply(lambda r: r.split('/')[-1])\n",
    "    df = df[df[\"Pages\"].isin(['select', 'checkout', 'confirmation'])]\n",
    "    df['Timestamp'] = df['Timestamp'].apply(lambda r: pd.to_datetime(r, errors='coerce'))\n",
    "    df['Links'] = df['Links'].apply(str)\n",
    "    return df\n",
    "\n",
    "def clean_data(aString):\n",
    "    aString = aString.replace('|Missing', '')\n",
    "    aString = re.sub('[[][A-Za-z0-9]*[]]', '', aString)  # removes things like [novconvalueinhbx]\n",
    "    aString = re.sub('[|]+', '|', aString)\n",
    "    return aString\n",
    "\n",
    "def pageExit(col):\n",
    "    return col.split('|')[-1]\n",
    "\n",
    "def linkExit(row):\n",
    "    if row['Exit_Page'] == 'checkout':\n",
    "        return  row['checkout'].split('|')[-1]\n",
    "    elif row['Exit_Page'] == 'select':\n",
    "        return row['select'].split('|')[-1]\n",
    "    else:\n",
    "        return 'Not Available'\n",
    "    \n",
    "def trigram(row):\n",
    "    if row['Exit_Page'] == 'checkout':\n",
    "        return  '|'.join(row['checkout'].split('|')[-4:-1])\n",
    "    elif row['Exit_Page'] == 'select':\n",
    "        return '|'.join(row['select'].split('|')[-4:-1])\n",
    "    else:\n",
    "        return 'Not Available'\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = s.split('|')\n",
    "    tokens = [t for t in tokens if len(t) > 3]\n",
    "    return tokens\n",
    "\n",
    "########################################################### Algorithm for generating the sequence ##########################################\n",
    "\n",
    "def page_sequence():\n",
    "    df = get_data('/usr/hdp/datasets/CMB_Digital_Analytics/INPUT/cja-ds-prospects.tsv')\n",
    "    \n",
    "    tot_uni_ses_id = df['Session_ID'].nunique()  # total unique session ids\n",
    "    ses_dict = {}  # a dictionary with key as session_id\n",
    "    \n",
    "    for ses in range(tot_uni_ses_id):\n",
    "        act_ses_id = df['Session_ID'].unique()[ses] # active session id\n",
    "        df_act_ses = df[df['Session_ID'] == act_ses_id].sort_values('Timestamp', ascending=True) # sorted df for that session_id\n",
    "        \n",
    "        num_of_rec = len(df_act_ses)  # total records in the active data set\n",
    "        cntr = 0 # page increment\n",
    "        page_collection = df_act_ses['Pages'].values[0] # by default starting with the first page\n",
    "        link_collection = '' # link sequence\n",
    "        page_dict = {} # a dictionary with key as page (cc)\n",
    "        link_counter = 0 # this counter is used to create the page_dict for a given page (cc)\n",
    "        \n",
    "        while num_of_rec > 0:\n",
    "            this_page = df_act_ses['Pages'].values[cntr]\n",
    "            \n",
    "            if cntr == len(df_act_ses)-1:  # to avoid the increment of the last record, or get out-of-bound\n",
    "                next_page = df_act_ses['Pages'].values[cntr]\n",
    "                link_counter += 1 # for the last record of the df_act_res\n",
    "            else:\n",
    "                next_page = df_act_ses['Pages'].values[cntr+1]\n",
    "                \n",
    "            if this_page != next_page:  # evaluates when the page type changes in the sequence\n",
    "                page_collection = page_collection + '|' + next_page\n",
    "                link_collection = link_collection + '|' + df_act_ses['Links'].values[cntr].split('|')[-1]\n",
    "                link_counter += 1 # this is where the page changes and the dict needs to be written\n",
    "            else:\n",
    "                link_collection = link_collection + '|' + df_act_ses['Links'].values[cntr].split('|')[-1]\n",
    "                \n",
    "            # Writing the page_dict\n",
    "            if link_counter > 0:\n",
    "                link_collection = clean_data(link_collection)\n",
    "                page_collection = clean_data(page_collection)\n",
    "                page_dict['PageSequence'] = page_collection\n",
    "                if 'confirmation' in page_collection.split('|'):\n",
    "                    page_dict['OrderPlaced'] = 1\n",
    "                else:\n",
    "                    page_dict[\"OrderPlaced\"] = 0\n",
    "                    \n",
    "                if this_page not in page_dict:\n",
    "                    page_dict[this_page] = link_collection\n",
    "                    link_collection = ''\n",
    "                else:\n",
    "                    prev_page_collection = page_dict[this_page]\n",
    "                    page_dict[this_page] = prev_page_collection + link_collection\n",
    "                    link_collection = ''\n",
    "                    link_counter = 0 # reseting the link\n",
    "                \n",
    "            cntr += 1  # increments the page\n",
    "            num_of_rec -= 1  # decrment for the while loop\n",
    "            \n",
    "        ses_dict[act_ses_id] = page_dict # a dictionary within a dictionary\n",
    "    return ses_dict\n",
    "\t\n",
    "########################################################### Reporting the Sequence Stats  #####################################################\n",
    "\n",
    "def write_data():\n",
    "    pages_sessions = page_sequence()\n",
    "    page_seq_df = pd.DataFrame.from_dict({i: pages_sessions[i] for i in pages_sessions.keys()}, orient='index')\n",
    "    page_seq_df.index.name = 'Session_ID'\n",
    "    #page_seq_df = pd.DataFrame(pages_sessions.items(), columns=['Session_ID', 'PageSesquence'])\n",
    "    #page_seq_df['OrderPlaced'] = page_seq_df['PageSequence'].apply(lambda r: 1 if 'confirmation' in r.split('|') else 0)\n",
    "    #page_seq_df.to_csv('/usr/hdp/datasets/CMB_Digital_Analytics/page_seq4.csv', sep='\\t', encoding='utf-8')\n",
    "    return page_seq_df\n",
    "\n",
    "def exitStats():\n",
    "    full_df = write_data()\n",
    "    NO_df = full_df[full_df['OrderPlaced'] == 0]\n",
    "    NO_df['Exit_Page'] = NO_df['PageSequence'].apply(pageExit)\n",
    "    NO_df['Exit_Link'] = NO_df.apply(linkExit, axis = 1) # axis = 1 is by row\n",
    "    NO_df['Exit_Trigram'] = NO_df.apply(trigram, axis = 1) # axis = 1 is by row\n",
    "    #NO_df.to_csv('/usr/hdp/datasets/CMB_Digital_Analytics/page_ONP.csv', sep='\\t', encoding='utf-8')\n",
    "    return NO_df\n",
    "\n",
    "# Writing the sequence-\n",
    "# NO_orders_df = exitStats()\n",
    "\n",
    "############################################################# The main function #############################################################\n",
    "\n",
    "def main():\n",
    "    print('\\n\\nThe exit statistics is being generated, please be patient ...\\n\\n')\n",
    "    NO_orders_df = exitStats()\n",
    "    \n",
    "    orig_stdout = sys.stdout\n",
    "    f = open('/usr/hdp/datasets/CMB_Digital_Analytics/OUTPUT/out.txt', 'w')\n",
    "    sys.stdout = f\n",
    "    \n",
    "    page_lookup = ['checkout', 'select']\n",
    "    \n",
    "    for u_exit_page in page_lookup:\n",
    "        print(\"============================= Below is the exit statistics for the \" + u_exit_page.upper() + \"  page =========================\")\n",
    "        print('\\n')\n",
    "        u_exit_df = NO_orders_df[NO_orders_df['Exit_Page'] == u_exit_page]\n",
    "        u_exit_df.replace('', np.nan, inplace=True)\n",
    "        u_exit_df.dropna()\n",
    "        top_exit_links = u_exit_df['Exit_Link'].value_counts().head(10)\n",
    "        top_exit_links_prnt_1 = u_exit_df['Exit_Link'].value_counts().head(30)\n",
    "        top_exit_links_prnt_2 = u_exit_df['Exit_Link'].value_counts()\n",
    "        print('=================================== Different sequence of ' + u_exit_page.upper() + ' exits =========================================== \\n')\n",
    "        print(np.round(100. * u_exit_df['PageSequence'].value_counts() / len(u_exit_df['PageSequence']), 2))\n",
    "        print('\\n')\n",
    "        print('=================================== The below are the top exit links for the '+ u_exit_page.upper() + ' exit page ===================== \\n')\n",
    "        print(100. * top_exit_links_prnt_1 / sum(top_exit_links_prnt_2))\n",
    "        print '\\n Total percentage of exits explained: ', np.round(sum(100. * top_exit_links_prnt_1 / sum(top_exit_links_prnt_2)), 2), '%'\n",
    "        print('\\n')\n",
    "        \n",
    "        print(\"========================================================================================================================\")\n",
    "        print('      ================ Customer interaction attributes for Link Exits in ' +  u_exit_page.upper() + ' page ========================')\n",
    "        print(\"========================================================================================================================\\n\")\n",
    "\n",
    "        for ind in top_exit_links.index:\n",
    "            token_dict = {}\n",
    "            u_exit_link_df = u_exit_df[u_exit_df['Exit_Link'] == ind]\n",
    "            u_exit_link_df = u_exit_link_df[u_exit_link_df['Exit_Trigram'].notnull()]\n",
    "            for exit_trigram in u_exit_link_df['Exit_Trigram'].values:\n",
    "                tokens = my_tokenizer(exit_trigram)\n",
    "                for token in tokens:\n",
    "                    if token not in token_dict:\n",
    "                        token_dict[token] = 1\n",
    "                    else:\n",
    "                        token_dict[token] += 1\n",
    "\n",
    "\n",
    "            print('================================Top clicks which resulted in ' + ind.upper() + ' link exit ===========================')\n",
    "            sorted_token_dict = sorted(token_dict.items(), key=operator.itemgetter(1), reverse = True)[0:10] # output is a tuple\n",
    "            name, values = zip(*sorted_token_dict) # unpacking the tuple\n",
    "            print(name)\n",
    "            print('\\n')\n",
    "        print(\"***************************************************************************************************************************\")\n",
    "        print(\"***************************************************************************************************************************\\n\\n\")\n",
    "        \n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()\n",
    "    \n",
    "    print(\"\\n\\nPrinting is completed.\")\n",
    "    print(\"Collect the report from /usr/hdp/datasets/CMB_Digital_Analytics/OUTPUT/\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
