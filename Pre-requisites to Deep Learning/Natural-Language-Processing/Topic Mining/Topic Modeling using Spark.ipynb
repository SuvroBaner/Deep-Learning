{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Set Spark Configuration\n",
    "\n",
    "import sys, os\n",
    "\n",
    "'''\n",
    "1. Sets Spark Home\n",
    "2. Sets Warehouse Location for Hive\n",
    "'''\n",
    "\n",
    "def fnInitConfig():\n",
    "    ## Setting Path for SPARK_HOME\n",
    "    os.environ[\"SPARK_HOME\"] = \"/apps/opt/applicaitons/spark-2.1.0-bin-hadoop2.7/\"\n",
    "    os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"python/lib\"\n",
    "    sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.4-src.zip\")\n",
    "    sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "    ## Context Configurations\n",
    "    #warehouse_location = '/apps/opt/applicaitons/datasets/chatAnalysis/'\n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    global spark\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('chatCategorizer') \\\n",
    "    .config('spark.master','local[*]') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = None\n",
    "spark = fnInitConfig()  # returning a Spark session\n",
    "print('Spark Session created...good to go')\n",
    "\n",
    "### Data Reading\n",
    "\n",
    "from __future__ import division, unicode_literals\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, IDF, CountVectorizer, IDFModel\n",
    "from pyspark.sql import functions as F\n",
    "%matplotlib inline\n",
    "\n",
    "## Read Data\n",
    "\n",
    "def fnReadData(spark, chat_type):\n",
    "    if chat_type == 'Speech':\n",
    "        FILEPATH = '/apps/opt/applicaitons/datasets/Datasets/raw_transcript_5000.txt'\n",
    "        SCRIPT = 'script'\n",
    "        \n",
    "        # below is pyspark.sql.dataframe.DataFrame\n",
    "        df_orig = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", \"\\t\").load(FILEPATH, header = True)\n",
    "        \n",
    "        # renaming of columns\n",
    "        oldColumns = df_orig.schema.names  # list of column names\n",
    "        newColumns = [\"mtn\", \"start_tm\", oldColumns[2], oldColumns[3], \"script\"]\n",
    "        df_orig = reduce(lambda df_orig, idx: df_orig.withColumnRenamed(oldColumns[idx], newColumns[idx]), \n",
    "                         range(len(oldColumns)), df_orig)\n",
    "        \n",
    "        # concatenating the text, group by 'mtn's\n",
    "        df_orig = df_orig.groupBy('mtn').agg(\n",
    "            F.min('start_tm').alias('start_tm'), \n",
    "            F.concat_ws(' ', F.collect_list('script')).alias('script'))\n",
    "        \n",
    "        # taking a copy of the initial df-\n",
    "        df = df_orig\n",
    "        \n",
    "        # selecting the required fields\n",
    "        df = df.select(['mtn', 'script'])\n",
    "        \n",
    "    elif chat_type == 'Chat':\n",
    "        FILEPATH = '/apps/opt/applicaitons/datasets/Datasets/campaigndetails.tsv'\n",
    "        SCRIPT = 'script'\n",
    "        \n",
    "        #query = 'select * from verizonwireless.IntegrationOnline_TelesalesJourney_chatV1'\n",
    "        \n",
    "        # below is pyspark.sql.dataframe.DataFrame\n",
    "        df_orig = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", \"\\t\").load(FILEPATH, header = True)\n",
    "        #df_orig = spark.sql(query)\n",
    "        \n",
    "        # renaming of columns\n",
    "        oldColumns = df_orig.schema.names  # list of column names\n",
    "        newColumns = [\"chat_id\", oldColumns[1], oldColumns[2], oldColumns[3], \"script\"]\n",
    "        df_orig = reduce(lambda df_orig, idx: df_orig.withColumnRenamed(oldColumns[idx], newColumns[idx]), \n",
    "                         range(len(oldColumns)), df_orig)\n",
    "        \n",
    "        # removing the duplicates\n",
    "        df_orig = df_orig.drop_duplicates(subset=['chat_id'])\n",
    "        \n",
    "        # taking a copy of the original df\n",
    "        df = df_orig\n",
    "        \n",
    "        # selecting the required fields\n",
    "        df = df.select(['chat_id', 'script'])\n",
    "        \n",
    "    else:\n",
    "        SCRIPT = 'script'\n",
    "        query = 'select * from verizonwireless.IntegrationOnline_TelesalesJourney_chatV1'\n",
    "        df_orig = spark.sql(query)\n",
    "        \n",
    "         # renaming of columns\n",
    "        oldColumns = df_orig.schema.names  # list of column names\n",
    "        newColumns = [\"chat_id\", \"script\", oldColumns[2], oldColumns[3]]\n",
    "        df_orig = reduce(lambda df_orig, idx: df_orig.withColumnRenamed(oldColumns[idx], newColumns[idx]), \n",
    "                         range(len(oldColumns)), df_orig)\n",
    "        \n",
    "        # removing the duplicates\n",
    "        df_orig = df_orig.drop_duplicates(subset=['chat_id'])\n",
    "        \n",
    "        # taking a copy of the original df\n",
    "        df = df_orig\n",
    "        \n",
    "        # selecting the required fields\n",
    "        df = df.select(['chat_id', 'script'])\n",
    "    \n",
    "    return df_orig, df, SCRIPT\n",
    "\n",
    "df_orig, final_df, SCRIPT = fnReadData(spark, chat_type = 'Hive')\n",
    "\n",
    "### Data Cleaning and Tokenization\n",
    "\n",
    "# Creating objects for lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "bogus_words = ['hi', 'ok', 'okay', 'yes', 'customer', 'hello', 'hmm', 'hmmm', 'hmmmm', 'thnx', 'ty', 'ohk', 'hey', \n",
    "           'tq', 'hmmk', 'blah', 'bla', 'tku', 'thank', 'thanks', 'nt', 'u', 'e', 'en', 'xx', 't', 'yeah', 'youre', \n",
    "           'yeah', 'youre', 'thats', 'thing', 'dont', 'something', 'maam', 'everything', 'name', 'dont', 'anything',\n",
    "          'didnt', 'mean', 'look', 'guy', 'kind', 'moment', 'wife', 'yesterday', 'sorry', 'not', 'can', 'right',\n",
    "          'gosh', 'today', 'amir', 'cent', 'forty', 'please', 'system', 'person', 'people', 'can', 'not',\n",
    "          'doe', 'number', 'hi', 'ok', 'okay', 'yes', 'customer', 'hello', 'hmm', 'hmmm', 'hmmmm', 'thnx', 'ty', 'ohk',\n",
    "         'hey', 'tq', 'hmmk', 'blah', 'bla', 'tku', 'none', 'None', 'haha', 'gt', 'etc', 'hehe', 'se',\n",
    "        'yo', 'ya', 'omg', 'technically', 'gracias', 'ah', 'un', 'ci', 'certainly', 'ki', 'dot', 'ti',\n",
    "        'etc', 'regularly', 'outright', 'bc', 'truly', 'nearly', 'txt', 'significantly', 'per', 'yep',\n",
    "        'yup', 'nd', 'immediately', 'ma', 'tho', 'umm', 'np', 'gotcha', 'zy', 'lately', 'extremely',\n",
    "        'ho', 'enjoy', 'youre', 'youve', 'yea', 'yeah', 'heck', 'thru', 'ha', 'sec', 'yup', 'na',\n",
    "        'thanks', 'thank', 'nt', 'en', 'xx', 'dont', 'thats', 'something', 'maam', 'everything',\n",
    "        'didnt', 'mean', 'look', 'guy', 'kind', 'moment', 'yesterday', 'sorry', 'not', 'can', 'right',\n",
    "        'gosh', 'today', 'amir', 'cent', 'please', 'person', 'people', 'can', 'not', 'doe', 'oh',\n",
    "        'got', 'go', 'like', 'guess', 'much', 'far', 'im', 'still', 'actually', 'really', 'awesome',\n",
    "        'said', 'hope', 'sure', 'sorry', 'because', 'let', 'morning', 'curious', 'bit', 'cool', 'ti',\n",
    "        'well', 'nope', 'also', 'basically', 'th', 'null', 'NULL', 'Null', 'its', 'nothing', 'pa', 'mike', 'died',\n",
    "          'parent', 'isnt', 'tomorrow', 'husband', 'yet', 'want', 'town', 'family', 'years', 'them', 'meet', 'ampamp',\n",
    "          'stephen', 'till', 'daughter', 'done', 'get', 'you']\n",
    "\n",
    "def fn_tokenizer(s):\n",
    "    s = re.sub('x{2,}', '', s)  # remove masking\n",
    "    s = re.sub('[*]', '', s) # remove masking\n",
    "    s = re.sub('\\d', '', s) # remove digits\n",
    "    s = re.sub(' +', ' ', s) # remove multiple spaces\n",
    "    s = s.lower() # making everyhing lower case\n",
    "    s = ' '.join(word for word in s.split(' ') if len(word) > 3)\n",
    "    s = ' '.join(wordnet_lemmatizer.lemmatize(word) for word in s.split(' '))\n",
    "    s = ' '.join(lemmatizer.lemmatize(word) for word in s.split(' '))\n",
    "    pos_tuple = pos_tag(s.split()) # parts of speech tagging\n",
    "    s = ' '.join(word for word, pos in pos_tuple if pos in ['NN', 'NNP', 'NNS', 'NNPS']) # returning only noun\n",
    "    s = re.sub('''[,.!?:;\\\"']''', '', s) # # remove punctuations\n",
    "    s = ' '.join(word for word in s.split(' ') if word not in bogus_words) # removing bogus words\n",
    "    \n",
    "    return (s) # the processed script\n",
    "\n",
    "def fnCleanData(final_df, SCRIPT):\n",
    "    udf_cleanData = udf(fn_tokenizer, StringType()) # an UDF created for fn_tokenizer (user_function, return String Type)\n",
    "    udf_rem_rec = udf(lambda x: len(x) > 0, BooleanType()) # an UDF to remove blank cell values of SCRIPT\n",
    "    \n",
    "    final_df = final_df.withColumn(\"script\", udf_cleanData(final_df[SCRIPT]))\n",
    "    final_df = final_df.filter(udf_rem_rec(final_df[SCRIPT]))\n",
    "    return final_df\n",
    "\n",
    "final_df = fnCleanData(final_df, SCRIPT)\n",
    "\n",
    "### TF-IDF and Topic Modeling\n",
    "\n",
    "# Term Frequency : Number of times word appears in a document blob, \n",
    "# normalized by dividing by the total number of words in the blob\n",
    "# We use TextBlob for breaking up the text into words and getting the word counts.\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "# Returns the number of documents containing word.\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "# Computes inverse document frequency, which measures how common a word is among all documents in the bloblist.\n",
    "# The more common the word is, lower the idf. We take the ratio of the total number of documents to the number of \n",
    "# documents containing word, then take a log of that. Add 1 to the divisor to prevent division by 0.\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "# It computes the TF-IDF score\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "def createBlob(line):\n",
    "    return (line[0], tb(line[1]))\n",
    "\n",
    "def findTF(x):\n",
    "    indx = x[0] # chat_id\n",
    "    blob = x[1] # script\n",
    "    top_words = []\n",
    "    high_info_words = []\n",
    "    scores = {word: tf(word, blob) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key = lambda x: x[1], reverse = True)\n",
    "    threshold = np.nanpercentile([pair[1] for pair in sorted_words], 25)\n",
    "    top_words = [pair[0] for pair in sorted_words if pair[1] > threshold]\n",
    "    for ind, word in enumerate(sorted(blob.words)):\n",
    "        if word in sorted(top_words):\n",
    "            high_info_words.append(word)\n",
    "    \n",
    "    return (indx, high_info_words)\n",
    "    \n",
    "def fn_topwords(df):\n",
    "    bloblist_dict_rdd = df.rdd.map(lambda x: createBlob(x)) \\\n",
    "                        .map(lambda x: findTF(x))  # creates a blob and then computes the TF for each doc\n",
    "    \n",
    "    pre_topic_dict = bloblist_dict_rdd.collectAsMap() # converts the list of tuples to a dictionary {'chat_id': ['tok1', 'tok2]}\n",
    "    return pre_topic_dict\n",
    "\n",
    "pre_topic_dict = fn_topwords(final_df)\n",
    "\n",
    "def pickWords(row, dct):\n",
    "    idx = row[0]  # index\n",
    "    x = row[1] # word tokens for than index\n",
    "    thresh = np.nanpercentile([v for k, v in dct.items() if k in x], 25)\n",
    "    high_info_words = [a for a, b in [(k, v) for k, v in dct.items() if k in x] if b >= thresh]\n",
    "    \n",
    "    high_info_list = []\n",
    "    for word in x.split(' '):\n",
    "        if word in high_info_words:\n",
    "            high_info_list.append(word)\n",
    "    return (idx, high_info_list)  # returning index and list of high info words\n",
    "\n",
    "def compute_TF_IDF(df):\n",
    "    tagged_df = df # will perform TF-IDF on tagged_df\n",
    "    tagged_df = tagged_df.withColumn(SCRIPT, split(SCRIPT, ' ')) # making SCRIPT as a Array Type\n",
    "    \n",
    "    # Performing TF-IDF using pyspark.ml.feature\n",
    "    cv = CountVectorizer(inputCol=SCRIPT, outputCol=\"countVec\", vocabSize=100000, minDF=10)\n",
    "    cvModel = cv.fit(tagged_df)\n",
    "    tagged_df = cvModel.transform(tagged_df)\n",
    "    \n",
    "    ## IDF : the input is the sparse vector calculated in the previous step\n",
    "    idf = IDF(inputCol=\"countVec\", outputCol=\"tfIdfVec\") # Compute the Inverse Document Frequency (IDF) given a collection of documents.\n",
    "    idfModel = idf.fit(tagged_df) # idfModel.idf creates a dense vector for each word (token).\n",
    "    \n",
    "    # cvModel.vocabulary has all the unique words. This is exactly same as the no. of words in the IDF.\n",
    "    wordToIndexDict ={} # mapping of indices 0, 1, 2, ... with the words 'window', 'chat', 'internet'\n",
    "    for i,j in enumerate(cvModel.vocabulary):\n",
    "        wordToIndexDict[i] = j\n",
    "    vecSize=len(cvModel.vocabulary)\n",
    "    \n",
    "    idfDict ={} # mapping of the indices 0, 1, 2, ... with the IDF score  0.4291, 0.5971, 1.1868 ...\n",
    "    for i,j in enumerate(idfModel.idf):\n",
    "        idfDict[i] = j\n",
    "        \n",
    "    # Now we need to tie up the words and the IDF score\n",
    "    wordtoIdfDict = {} # mapping of words like 'window', 'chat', 'internet' and their IDF scores like 0.4291, 0.5971, 1.1868 ...\n",
    "    for i,j in zip(wordToIndexDict,idfDict):\n",
    "        wordtoIdfDict[wordToIndexDict[i]] = idfDict[j]\n",
    "        \n",
    "    df_rdd = df.rdd.map(lambda row: pickWords(row, wordtoIdfDict))\n",
    "    \n",
    "    pre_topic_dict = df_rdd.collectAsMap() # converts the list of tuples to a dictionary {'chat_id': ['tok1', 'tok2]}\n",
    "    return pre_topic_dict\n",
    "    \n",
    "    return pre_topic_dict\n",
    "\n",
    "#pre_topic_dict2 = compute_TF_IDF(final_df)\n",
    "\n",
    "def optimize_topic(lsamodel, doc_term_matrix):\n",
    "    Lsi_2d_data = []  # output of svd will be stored here\n",
    "\n",
    "    #for v in lsamodel[doc_term_matrix]:\n",
    "    #    print(v)\n",
    "    \n",
    "    for vector in lsamodel[doc_term_matrix]:\n",
    "        if len(vector) != 2:\n",
    "            continue\n",
    "        Lsi_2d_data.append((vector[0][1], vector[1][1]))\n",
    "        \n",
    "    # Next I clustered the points in the reduced 2D LSI space using KMeans, varying the number of clusters (K) from 1 to 10\n",
    "    # The objective function used is the Inertia of the cluster, defined as the sum of squared differences of each point \n",
    "    # to its cluster centroid\n",
    "    \n",
    "    MAX_K = 10\n",
    "\n",
    "    X = Lsi_2d_data\n",
    "    ks = range(1, MAX_K + 1)\n",
    "\n",
    "    inertias = np.zeros(MAX_K)\n",
    "    diff = np.zeros(MAX_K)\n",
    "    diff2 = np.zeros(MAX_K)\n",
    "    diff3 = np.zeros(MAX_K)\n",
    "\n",
    "    for k in ks:\n",
    "        kmeans = KMeans(k).fit(X)\n",
    "        inertias[k - 1] = kmeans.inertia_\n",
    "        # first difference    \n",
    "        if k > 1:\n",
    "            diff[k - 1] = inertias[k - 1] - inertias[k - 2]\n",
    "        # second difference\n",
    "        if k > 2:\n",
    "            diff2[k - 1] = diff[k - 1] - diff[k - 2]\n",
    "        # third difference\n",
    "        if k > 3:\n",
    "            diff3[k - 1] = diff2[k - 1] - diff2[k - 2]\n",
    "\n",
    "    elbow = np.argmin(diff3[3:]) + 3\n",
    "    \n",
    "    num_of_topics = ks[elbow]\n",
    "    \n",
    "    plt.plot(ks, inertias, \"b*-\")\n",
    "    plt.plot(ks[elbow], inertias[elbow], marker='o', markersize=12,\n",
    "             markeredgewidth=2, markeredgecolor='r', markerfacecolor=None)\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The Number of Optimal Topics: \", num_of_topics)\n",
    "    \n",
    "    #X = a\n",
    "    #kmeans = KMeans(NUM_TOPICS).fit(X)\n",
    "    #y = kmeans.labels_\n",
    "\n",
    "    #colors = [\"b\", \"g\", \"r\", \"m\", \"c\"]\n",
    "    #for i in range(np.array(X).shape[0]):\n",
    "    #    plt.scatter(X[i][0], X[i][1], c=colors[y[i]], s=10)    \n",
    "    #plt.show()\n",
    "    \n",
    "    return num_of_topics\n",
    "\n",
    "def topic_modeling(pre_topic_dict):\n",
    "    doc_clean = []\n",
    "    for k, v in pre_topic_dict.items():\n",
    "        doc_clean.append(v)\n",
    "        \n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    \n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    \n",
    "    # I didn't know, how many topics this corpus should yield, so I decided to compute this by reducing the features to \n",
    "    # two dimensions, then clustering the points for different values of K (number of clusters) to find an optimum value.\n",
    "    # To project the vectors in a corpus to a different coordinate space (say 2D), we will use LSA (Latent Semantic Analysis).\n",
    "    # This is Singular Value Decomposition (SVD)\n",
    "    \n",
    "    Lsi = gensim.models.LsiModel\n",
    "    \n",
    "    # project to 2 dimensions for visualization\n",
    "    lsamodel = Lsi(doc_term_matrix, id2word=dictionary, num_topics=2)\n",
    "    \n",
    "    num_of_topics = optimize_topic(lsamodel, doc_term_matrix)\n",
    "    #num_of_topics = 10\n",
    "    \n",
    "    # Running LDA Model\n",
    "    # Next step is to create an object for LDA model and train it on Document-Term matrix. \n",
    "    # Creating the object for LDA model using gensim library\n",
    "    #Lda = gensim.models.ldamodel.LdaModel\n",
    "    Lda = gensim.models.ldamulticore.LdaMulticore\n",
    "    \n",
    "    # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=num_of_topics, id2word = dictionary, passes=1, chunksize = 1000)\n",
    "    \n",
    "    return ldamodel, doc_term_matrix, num_of_topics\n",
    "\n",
    "lda_model, dtm, num_of_topics = topic_modeling(pre_topic_dict)\n",
    "\n",
    "def find_topic(aList):\n",
    "    num_topics = len(aList)\n",
    "    mean_top_prob = np.mean([lst_itm[1] for lst_itm in aList]) - .01  # to take the border line topics (1 % tolerance)\n",
    "    if num_topics == 1:\n",
    "        return [aList[0][0]]  # return the only topic\n",
    "    elif num_topics == 2:\n",
    "        return [lst_itm[0] for lst_itm in aList if lst_itm[1] >= mean_top_prob + .05] # one topic should have at least 55% probability\n",
    "    else:\n",
    "        return [lst_itm[0] for lst_itm in aList if lst_itm[1] >= mean_top_prob*1.2] # one topic atleast 1.2 the mean prob.\n",
    "    \n",
    "\n",
    "def topic_doc_map(lda_model, dtm, num_of_topics):\n",
    "    lda_corpus = lda_model[dtm]\n",
    "    \n",
    "    topic_doc_dict = {}\n",
    "    for doc, topic in enumerate(lda_corpus):\n",
    "        #topic_doc_dict[doc] = sorted(topic, key = lambda x: x[1], reverse = True)[0][0]\n",
    "        topic_doc_dict[doc] = find_topic(topic)\n",
    "        \n",
    "    topic_word_dict = {}\n",
    "    num_of_topics = num_of_topics\n",
    "\n",
    "    all_topics = []\n",
    "    for topic in range(num_of_topics):\n",
    "        all_tokens = lda_model.get_topic_terms(topic) # it's a tuple (word_id, weight) for a given topic\n",
    "        wt_thresh = np.percentile([tup[1] for tup in all_tokens], 25) # setting a weight threshold\n",
    "        selected_tokens = [tup[0] for tup in all_tokens if tup[1] > wt_thresh]\n",
    "        total_tokens = len(selected_tokens)\n",
    "        all_topics.append((topic, lda_model.print_topic(topic, total_tokens))) # final topic and number of tokens list\n",
    "        \n",
    "    all_topics = [(tup[0], (lambda a: re.findall(\"[a-zA-Z]+\", a))(tup[1])) for tup in all_topics]  # removing the weights\n",
    "    \n",
    "    for topic in all_topics:\n",
    "        topic_word_dict[topic[0]] = topic[1]\n",
    "        \n",
    "    return topic_doc_dict, topic_word_dict\n",
    "\n",
    "topic_doc_dict, topic_word_dict = topic_doc_map(lda_model, dtm, num_of_topics)\n",
    "\n",
    "def topic_extract(r):\n",
    "    num_top = len(r)\n",
    "    if num_top == 0:\n",
    "        return 'OTHER_TOPIC'\n",
    "    elif num_top == 1:\n",
    "        return topic_word_dict[r[0]]\n",
    "    elif num_top > 1:\n",
    "        return dict((el, topic_word_dict[el]) for el in r)\n",
    "    \n",
    "def present_data(final_df, topic_doc_dict, topic_word_dict):\n",
    "    final_df = final_df.toPandas()\n",
    "    topic_df = pd.concat((final_df, pd.Series(topic_doc_dict, name='topic')), axis = 1)\n",
    "    topic_df['topic_tokens'] = topic_df['topic'].apply(topic_extract)\n",
    "    topic_df.to_csv('final_output_text_topic_new.csv')\n",
    "    return topic_df\n",
    "\n",
    "present_data(final_df, topic_doc_dict, topic_word_dict).head(5)\n",
    "\n",
    "topic_word_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
