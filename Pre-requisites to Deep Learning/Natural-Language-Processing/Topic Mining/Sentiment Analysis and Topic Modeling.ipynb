{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Set Spark Configurations\n",
    "\n",
    "import sys, os\n",
    "\n",
    "'''\n",
    "1. Sets Spark Home\n",
    "2. Sets Warehouse Location for Hive\n",
    "'''\n",
    "\n",
    "def fnInitConfig():\n",
    "    ## Setting Path for SPARK_HOME\n",
    "    os.environ[\"SPARK_HOME\"] = \"/apps/opt/applicaitons/spark-2.1.0-bin-hadoop2.7/\"\n",
    "    os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"python/lib\"\n",
    "    sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.4-src.zip\")\n",
    "    sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "    ## Context Configurations\n",
    "    #warehouse_location = '/apps/opt/applicaitons/datasets/chatAnalysis/'\n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    global spark\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('chatCategorizer') \\\n",
    "    .config('spark.master','local[*]') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    #.config('spark.master','local[*]') \\\n",
    "    #.config('spark.cores.max', '8') \\\n",
    "    #.config('spark.driver.memory','12g') \\\n",
    "    #.config('spark.executor.cores', '7') \\\n",
    "    #.config('spark.driver.cores','2') \\\n",
    "    #.config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    #.config('spark.master', 'spark://ip-10-34-42-44.ebiz.verizon.com:7077')\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = None\n",
    "spark = fnInitConfig()  # returning a Spark session\n",
    "print('Spark Session created...good to go')\n",
    "\n",
    "### Parameters to pass\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "path = str(sys.argv[1])\n",
    "filename = str(sys.argv[1]).split('/')[-1].split('.')[0]\n",
    "outputFolder = str(sys.argv[2])\n",
    "\n",
    "# Hardcoding it for the time being-\n",
    "\n",
    "path = \"/apps/opt/applicaitons/datasets/Datasets/feb_echat_for_vdsi_masked3.csv\"\n",
    "outputFolder = \"/apps/opt/applicaitons/datasets/NLP/SuvroChatAnalysis\"\n",
    "columnsList = ['chatID','customercontent', 'chatcategory'] ## Expects ChatID as first column and chatcontent a second\n",
    "filename = 'SUVRO'\n",
    "\n",
    "starttime = datetime.now()\n",
    "\n",
    "### Creating Schema\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# StructType : The data type representing rows. A StructType object comprises a list of StructFields.\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"chatID\", StringType()), \n",
    "    StructField(\"customercontent\", StringType()),\n",
    "    StructField(\"chatcategory\", StringType())\n",
    "])\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "from string import strip\n",
    "import numpy as np\n",
    "\n",
    "def removeMasking(line):\n",
    "    try:\n",
    "        #convert content to string since Spark does not have replace method\n",
    "        # line[1] is the \"customercontent\" column. One row at a time comes in.\n",
    "        # line[2] is the \"chatcategory\" column. One row at a time comes in.\n",
    "        a = line[1]\n",
    "        b = line[2]\n",
    "        #c = format(line[0], 'f')\n",
    "        #c = float(line[0])\n",
    "        \n",
    "        #remove punctuations\n",
    "        a = re.sub('[^a-zA-Z0-9]+', ' ', a.replace('*',\"\").replace('-',\"\")).strip()\n",
    "        \n",
    "        #remove multiple white spaces\n",
    "        a = re.sub('\\s+', ' ', a.replace('*',\"\").replace('-',\"\")).strip()\n",
    "        \n",
    "        # remove all spaces in chatcategory\n",
    "        b = b.replace(\" \", \"\")\n",
    "        \n",
    "        #the zero indicates no parsing error, 1 means ascii to unicode conversion error\n",
    "        return (line[0], a, b, 0)\n",
    "    except AttributeError:\n",
    "        return (line[0], a, b, 1)\n",
    "    \n",
    "def fnReadData(path, columnsList, spark, schema):\n",
    "    # returns a data frame with a custom schema created above.\n",
    "    cc_df = spark.read.csv(path=path, inferSchema=False, header=True,schema=schema) \n",
    "    \n",
    "    # converting the dataframe into an RDD. I think we can also omit the select statement.\n",
    "    cc_df = cc_df.select(columnsList[0],columnsList[1], columnsList[2]).rdd\n",
    "    \n",
    "    # cc_df.map() : it returns a new RDD by applying a function to each element of this RDD\n",
    "    # here x: is one row at a time which are the elements of the main RDD\n",
    "    cc_df = cc_df.map(lambda x: removeMasking(x))\n",
    "    \n",
    "    # Filtering the one with no error\n",
    "    cc_df = cc_df.filter(lambda x: x[3] == 0)\n",
    "    return cc_df\n",
    "\n",
    "cc_df = fnReadData(path, columnsList, spark, schema)\n",
    "cc_map_df = fnReadData(path, columnsList, spark, schema)\n",
    "\n",
    "print(cc_df.count())\n",
    "endtime = datetime.now()\n",
    "timeElapsed = (endtime-starttime).total_seconds()\n",
    "print('Time Taken : ' + str(int(timeElapsed/60)) + ' mins')\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "# give a sentiment intensity scores to sentences\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentScore(line):\n",
    "    # return engagement id, chat, aggregate sentiment score and existing customer\n",
    "    return (line[0], line[1], line[2], sid.polarity_scores(line[1])['compound'])\n",
    "\n",
    "def fnSentimentAnalyzer(cc_df):\n",
    "    cc_df = cc_df.map(lambda x: sentScore(x))\n",
    "    return cc_df\n",
    "\n",
    "cc_df = fnSentimentAnalyzer(cc_df)\n",
    "print('---> Done Sentiment Analysis !!!')\n",
    "print(cc_df.count())\n",
    "\n",
    "endtime = datetime.now()\n",
    "timeElapsed = (endtime-starttime).total_seconds()\n",
    "print('Time Taken : ' + str(int(timeElapsed/60.0)) + ' min')\n",
    "\n",
    "### POS Tagging\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "##POS tagging\n",
    "def returnNnAdj(line):\n",
    "    nounList = []\n",
    "    for x in line:\n",
    "        if x[1] in ['NN', 'NNP', 'NNS', 'ADJ']:\n",
    "            nounList.append(x[0])\n",
    "    return nounList\n",
    "\n",
    "##Tagging sentence as Complaint or Query\n",
    "def sentLabel(x):\n",
    "    if x <= 0.25:  # any thing less than equals to 0 is 'Complaint' (negative) and above it is a 'Query' (query)\n",
    "        return 'complaint'\n",
    "    else:\n",
    "        return 'query'\n",
    "    \n",
    "##Removing Junk words \n",
    "def removeBoguswords(tokens):\n",
    "    \n",
    "    #these are responses by customers which are concatenated with the last or first word of another sentence\n",
    "    patternsToSearch = ['hi', 'ok', 'okay', 'yes', 'customer', 'hello']\n",
    "    \n",
    "    stopEnglish = stopwords.words('english')\n",
    "    \n",
    "    #convert to lower case and remove stop words\n",
    "    tokens = [upperCaseWord.lower() for upperCaseWord in tokens if upperCaseWord.lower() not in stopEnglish]\n",
    "\n",
    "    for pattern in patternsToSearch:\n",
    "        tokens = [word for word in tokens if not re.findall(pattern=pattern,string=word, flags=re.IGNORECASE)]       \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def fnPOSTagger(cc_df):\n",
    "    cc_df = cc_df \\\n",
    "                .filter(lambda x: x[1] != '') \\\n",
    "                .map(lambda x: (x[0], pos_tag(word_tokenize(x[1])), x[2], sentLabel(x[3]))) \\\n",
    "                .map(lambda x: (x[0], returnNnAdj(x[1]), x[2], x[3])) \\\n",
    "                .map(lambda x: (x[0],removeBoguswords(x[1]),x[2], x[3])) \\\n",
    "                .filter(lambda x: len(x[1]) > 2)\n",
    "    return cc_df\n",
    "\n",
    "# only selects the chat which has some values\n",
    "# (chatid, pos of tokens, label)\n",
    "# returns nouns and adjectives\n",
    "# removing the bogus words from the tokens\n",
    "\n",
    "### Calling posTagging.py\n",
    "cc_df = fnPOSTagger(cc_df)\n",
    "print('---> Done POS Tagging !!!')\n",
    "print(cc_df.count())\n",
    "endtime = datetime.now()\n",
    "timeElapsed = (endtime-starttime).total_seconds()\n",
    "print('Time Taken : ' + str(int(timeElapsed)/60.0) + ' min')\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, IDF, CountVectorizer, IDFModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fnTfidf(cc_df):\n",
    "    \n",
    "    ## TFIDF\n",
    "    # type(preTfIdfSchema) -- pyspark.sql.types.StructType\n",
    "    preTfIdfSchema = StructType([StructField(name = 'chatID', dataType = StringType()),\n",
    "                                StructField(name = 'tokens', dataType = ArrayType(elementType = StringType())),\n",
    "                                 StructField(name = 'chatcategory', dataType = StringType()),\n",
    "                                StructField(name = 'label', dataType = StringType()), ])\n",
    "    \n",
    "    # type(cc_df.toDF(schema=preTfIdfSchema)) -- pyspark.sql.dataframe.DataFrame\n",
    "    tagged_df = cc_df.toDF(schema=preTfIdfSchema)\n",
    "    \n",
    "    ## CountVectorizer\n",
    "    # It takes tokens as an input and the output is Sparse Vector (which is a dictionary of the key : index (tokens) value : \n",
    "    # no. of times it has occurred)\n",
    "    # countVec=SparseVector(270, {0: 1.0, 1: 1.0, 3: 1.0, 4: 1.0, 6: 1.0, 13: 1.0, 16: 1.0, 39: 2.0, 81: 1.0, 92: 1.0})\n",
    "    cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"countVec\", vocabSize=100000, minDF=10)\n",
    "    cvModel = cv.fit(tagged_df)\n",
    "    tagged_df = cvModel.transform(tagged_df)\n",
    "    '''So, tagged_df contains the following columns-\n",
    "    chatID, tokens, label (query / complaint), SparseVector\n",
    "    [Row(chatID=u'e723769821540362226', \n",
    "    tokens=[u'name', u'fios', u'area', u'forge', u'thankswhen', u'problem', u'tv', u'service', u'thanks', \n",
    "    u'helpno', u'thank', u'name', u'chat', u'window'], label=u'query', \n",
    "    countVec=SparseVector(270, {0: 1.0, 1: 1.0, 3: 1.0, 4: 1.0, 6: 1.0, 13: 1.0, 16: 1.0, 39: 2.0, 81: 1.0, 92: 1.0}))]'''\n",
    "    \n",
    "    ## IDF : the input is the sparse vector calculated in the previous step\n",
    "    idf = IDF(inputCol=\"countVec\", outputCol=\"tfIdfVec\") # Compute the Inverse Document Frequency (IDF) given a collection of documents.\n",
    "    idfModel = idf.fit(tagged_df) # idfModel.idf creates a dense vector for each word (token). There are 270 tokens\n",
    "    \n",
    "    # cvModel.vocabulary has all the unique words. This is exactly same as the no. of words in the IDF.\n",
    "    wordToIndexDict ={} # mapping of indices 0, 1, 2, ... with the words 'window', 'chat', 'internet'\n",
    "    for i,j in enumerate(cvModel.vocabulary):\n",
    "        wordToIndexDict[i] = j\n",
    "    vecSize=len(cvModel.vocabulary)\n",
    "    \n",
    "    idfDict ={} # mapping of the indices 0, 1, 2, ... with the IDF score  0.4291, 0.5971, 1.1868 ...\n",
    "    for i,j in enumerate(idfModel.idf):\n",
    "        idfDict[i] = j\n",
    "    \n",
    "    # Now we need to tie up the words and the IDF score\n",
    "    wordtoIdfDict = {} # mapping of words like 'window', 'chat', 'internet' and their IDF scores like 0.4291, 0.5971, 1.1868 ...\n",
    "    for i,j in zip(wordToIndexDict,idfDict):\n",
    "        wordtoIdfDict[wordToIndexDict[i]] = idfDict[j]\n",
    "        \n",
    "    # Cut-off of TFIDF is 50 percentile-\n",
    "    cutoff_tfidf = np.percentile(pd.Series(idfModel.idf),50)\n",
    "    \n",
    "    # selecting the High Info Word-\n",
    "    highInfoWords = []\n",
    "    for i in wordtoIdfDict:\n",
    "        if wordtoIdfDict[i] >= cutoff_tfidf:\n",
    "            highInfoWords.append(i)\n",
    "            \n",
    "    del idfModel, cvModel, wordToIndexDict, idfDict, wordtoIdfDict, tagged_df,cv,preTfIdfSchema\n",
    "    \n",
    "    return highInfoWords\n",
    "\n",
    "highInfoWords = fnTfidf(cc_df)\n",
    "print('---> Done TFIDF !!!')\n",
    "endtime = datetime.now()\n",
    "timeElapsed = (endtime-starttime).total_seconds()\n",
    "print('Time Taken : ' + str(int(timeElapsed)/60.0) + ' min')\n",
    "\n",
    "### QCR : Query to Complaint Ratio\n",
    "\n",
    "## Function will return a list of tuples for each line in the RDD. \n",
    "## Where each tuple will be of the form:\n",
    "##(noun/adj i.e. a token,(complaint_count, query_count, total_count)) \n",
    "\n",
    "def nounCategorizer(line, highInfoWords):    \n",
    "    triplesList =[] \n",
    "    for word in line[1]:\n",
    "        if word in highInfoWords:  # only select the tokens which are part of the High Info Words corpus\n",
    "            if line[3] == 'complaint':\n",
    "                triplesList.append((word, (1, 0, 1)))\n",
    "            else:\n",
    "                triplesList.append((word, (0, 1, 1)))\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    return triplesList\n",
    "\n",
    "def qcr(line):\n",
    "    ratio = line[1][1]/ float(line[1][0]+1) # no. of query (25) / (no. of complaint(1) + 1) \n",
    "    if ratio > 1:\n",
    "        return float(np.log(ratio))  # for query.. the value will be higher\n",
    "    else:\n",
    "        return float(-np.log(1/(ratio+0.01)))  # for complaint.. the value will be lower\n",
    "\n",
    "## Function to score each sentence with qcr\n",
    "def qcrCompoundScorer(sentence,searchDict): # inputs : for a given chat : tokens , (word: QCR_score)\n",
    "    # extracting the score for each token which are present in the search dictionary\n",
    "    score = [searchDict[x.lower()] for x in sentence if searchDict.has_key(x.lower())]\n",
    "    if score:\n",
    "        normScore = np.sum(score)/len(score) # lower the normScore, more the complaint it is.\n",
    "        return float(normScore)\n",
    "    else:\n",
    "        return 9999\n",
    "    \n",
    "def normScore(x, mx, mn):\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "def fnQcrScoring(cc_df, highInfoWords):\n",
    "    '''1st O/p after nounCategorizer(): It's a list of list\n",
    "    [[(u'area', (0, 1, 1)), (u'problem', (0, 1, 1))],\n",
    " [(u'weeks', (0, 1, 1)), (u'today', (0, 1, 1)), (u'support', (0, 1, 1))]]'''\n",
    "    \n",
    "    '''2nd O/p: flapMap flattens the list as one list\n",
    "    Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.\n",
    "    [(u'area', (0, 1, 1)),\n",
    " (u'problem', (0, 1, 1)),\n",
    " (u'weeks', (0, 1, 1)),\n",
    " (u'today', (0, 1, 1))]'''\n",
    "    \n",
    "    '''3rd O/p: Aggregate the values of each key, using given combine functions and a neutral \"zero value\".\n",
    "    [(u'code', (1, 25, 26)),\n",
    " (u'money', (2, 32, 34)),\n",
    " (u'move', (1, 10, 11)),\n",
    " (u'questions', (1, 25, 26))]'''\n",
    "\n",
    "    nounCounts_rdd = cc_df \\\n",
    "                        .map(lambda x: nounCategorizer(x, highInfoWords)) \\\n",
    "                        .flatMap(lambda x: x) \\\n",
    "                        .aggregateByKey(zeroValue = (0, 0, 0), \\\n",
    "                                   seqFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2])), \\\n",
    "                                   combFunc = (lambda rdd1, rdd2: (rdd1[0] + rdd2[0], rdd1[1] + rdd2[1], rdd1[2] + rdd2[2])))\n",
    "                \n",
    "    qcr_rdd = nounCounts_rdd.map(lambda x: (x[0],qcr(x),x[1][2])) \n",
    "    # [(u'code', 2.5257286443082556, 26), (u'money', 2.367123614131617, 34)]\n",
    "    \n",
    "    ## Re-scoring the chats based on the QCR weights of the words in our search list\n",
    "    ## collecting the words into a dictionary\n",
    "    \n",
    "    searchDict = qcr_rdd \\\n",
    "                .map(lambda x: (x[0],x[1])) \\\n",
    "                .collectAsMap()\n",
    "    '''{u'access': 2.0794415416798357,\n",
    " u'acct': 1.6739764335716716,\n",
    " u'agent': 1.0986122886681098, ... }'''\n",
    "    \n",
    "    # Only take the chats where atleast one token matches with the high info words\n",
    "    mostNegativeChats = cc_df \\\n",
    "                        .filter(lambda x: len(set(x[1]).intersection(set(highInfoWords))) >= 1) \\\n",
    "                        .map(lambda x: (x[0], x[1], x[2], qcrCompoundScorer(x[1],searchDict)))\n",
    "    \n",
    "    mostNegativeChats_df = mostNegativeChats.toDF()\n",
    "    mx = mostNegativeChats_df.select(\"_4\").rdd.max()[0]\n",
    "    mn = mostNegativeChats_df.select(\"_4\").rdd.min()[0]\n",
    "    mostNegativeChats = mostNegativeChats.map(lambda x : (x[0], x[1], x[2], normScore(x[3], mx, mn)))\n",
    "    \n",
    "    del qcr_rdd,cc_df,nounCounts_rdd\n",
    "    \n",
    "    return mostNegativeChats # note it contains both negative and positive sentiments... it's incorrectly named\n",
    "\n",
    " # returns (chatid, chat tokens which are present in the high info words, category and  QCR compound score)\n",
    "\n",
    "mostNegativeChats = fnQcrScoring(cc_df,highInfoWords)\n",
    "print('---> Done QCR Computation !!!')\n",
    "print(mostNegativeChats.count())\n",
    "endtime = datetime.now()\n",
    "timeElapsed = (endtime-starttime).total_seconds()\n",
    "print('Time Taken : ' + str(int(timeElapsed)/60.0) + ' min')\n",
    "\n",
    "\n",
    "### Topic Extraction\n",
    "\n",
    "from pyspark.ml.clustering import LDA, LocalLDAModel\n",
    "\n",
    "def trainLDA(df, inputCol, k, maxIter=5,seed=0):\n",
    "        lda = LDA(k=k, maxIter=maxIter, featuresCol=inputCol).setSeed(seed)\n",
    "        return lda.fit(df.select(inputCol))\n",
    "    \n",
    "def pred_topic(x):\n",
    "    return int(np.argmax(x))\n",
    "\n",
    "def fnGetWords(indexList):\n",
    "    wordsList = []\n",
    "    for i in indexList:\n",
    "        wordsList.append(vocabDict[i])\n",
    "    return wordsList\n",
    "\n",
    "def fnGoodWords(line,highInfoWords):\n",
    "    wordList = []\n",
    "    for word in line[1]:\n",
    "        if word in highInfoWords:\n",
    "            wordList.append(word)\n",
    "    return wordList\n",
    "\n",
    "def fnCombWord(line):\n",
    "    combSentence = ''\n",
    "    for word in line[1]:\n",
    "        combSentence += ' '+word\n",
    "    return strip(combSentence)\n",
    "\n",
    "def fnLDAModel(mostNegativeChats, highInfoWords, folderToWrite, train):\n",
    "    mostNegativeChats = mostNegativeChats \\\n",
    "                                .map(lambda x: (x[0], fnGoodWords(x,highInfoWords), x[2], x[3]))\n",
    "        \n",
    "    preCVSchema = StructType([StructField(name = 'chatID', dataType=StringType()),\n",
    "                              StructField(name = 'tokens', dataType=ArrayType(elementType=StringType())),\n",
    "                              StructField(name = 'chatcategory', dataType=StringType()),\n",
    "                              StructField(name = 'avgQCRscore', dataType=FloatType()),])\n",
    "    \n",
    "    mostNegativeChats = mostNegativeChats.toDF(schema=preCVSchema)\n",
    "    \n",
    "    ## CountVectorizer\n",
    "    cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"countVec\", vocabSize=100000, minDF=10)\n",
    "    cvModel = cv.fit(mostNegativeChats)\n",
    "    mostNegativeChats = cvModel.transform(mostNegativeChats)\n",
    "    \n",
    "    '''\n",
    "    [Row(chatID=u'e723769821540362226', tokens=[u'area', u'problem'], chatcategory=u'Cat1', avgQCRscore=1.2384692430496216, countVec=SparseVector(203, {14: 1.0, 26: 1.0})),\n",
    " Row(chatID=u'e723769822320682275', tokens=[u'weeks', u'today', u'support'], chatcategory=u'Cat1', avgQCRscore=1.3547954559326172, countVec=SparseVector(203, {5: 1.0, 133: 1.0, 147: 1.0}))]\n",
    "    '''\n",
    "    if train == 'optimize':\n",
    "        ### The wrapper algorithm to build the model. Auto Identify the number of topics\n",
    "        k_best = 2\n",
    "        k_next = k_best\n",
    "        cutOff_counter = 0\n",
    "        perplexity = None\n",
    "\n",
    "        while cutOff_counter < 1:\n",
    "            lda_model = trainLDA(df=mostNegativeChats, inputCol='countVec',k=k_next)\n",
    "            current_perplexity = lda_model.logPerplexity(mostNegativeChats.select('countVec'))\n",
    "\n",
    "            if perplexity == None:\n",
    "                perplexity = current_perplexity\n",
    "                print \"--> k_best=%d, k_curr=%d, cutoff_counter=%d, k_next_perp=%.4f, k_best_perp=%.4f\" \\\n",
    "                    %(k_best,k_next,cutOff_counter,current_perplexity, perplexity)\n",
    "\n",
    "            else:\n",
    "                improvement = (current_perplexity - perplexity) / (perplexity *1.0)\n",
    "\n",
    "                if improvement >= -0.0099:\n",
    "                    cutOff_counter += 1\n",
    "\n",
    "                else:\n",
    "                    perplexity = current_perplexity\n",
    "                    cutOff_counter = 0\n",
    "                    k_best = k_next\n",
    "\n",
    "\n",
    "                print \"--> k_best=%d, k_curr=%d, cutoff_counter=%d, improvement%%=%.2f, k_next_perp=%.4f, k_best_perp=%.4f\" \\\n",
    "                                %(k_best,k_next,cutOff_counter,improvement,current_perplexity, perplexity)\n",
    "            k_next += 1\n",
    "        lda_model = trainLDA(df=mostNegativeChats, inputCol='countVec', k=k_best, maxIter=10,seed=0)\n",
    "        print('---> Model Built...!!!')\n",
    "\n",
    "    elif train == 'BuildOnce':\n",
    "        k_best = 5\n",
    "        lda_model = trainLDA(df=mostNegativeChats, inputCol='countVec', k=k_best, maxIter=5,seed=0)\n",
    "        print('---> Model Built...!!!')\n",
    "\n",
    "    else:\n",
    "        lda_model = LocalLDAModel.load('/apps/opt/applicaitons/datasets/chatAnalysis/ConsolidatedCode_V1/unsup_input_10k2017_04_11_2_50/lda_model/')\n",
    "        k_best = 5\n",
    "        print('---> Model picked up from disk...!!!')\n",
    "            \n",
    "    transformed = lda_model.transform(mostNegativeChats)\n",
    "    transformed = transformed\\\n",
    "                        .rdd \\\n",
    "                        .map(lambda x: (x[0], fnCombWord(x), x[2], x[3], pred_topic(x[5]))) \\\n",
    "                        .toDF(schema=StructType([\n",
    "                                        StructField(\"chatID\", StringType()),\n",
    "                                        StructField(\"sentence\", StringType()),\n",
    "                                        StructField(\"chatcategory\", StringType()),\n",
    "                                        StructField(\"avgQCRscore\", FloatType()),\n",
    "                                        StructField(\"predictedTopic\", IntegerType())]))\n",
    "        \n",
    "        \n",
    "    lda_model.write().overwrite().save(folderToWrite +'/lda_model')\n",
    "    \n",
    "    return k_best, transformed\n",
    "\n",
    "outputFolder = \"/apps/opt/applicaitons/datasets/NLP/SuvroChatAnalysis\"\n",
    "fileName = 'SUVRO'\n",
    "folderToWrite = outputFolder + '/'+ fileName + str(datetime.now().date())+'_'+ str(datetime.now().hour)+'_'+str(datetime.now().minute)+'/'\n",
    "k_best, transformed = fnLDAModel(mostNegativeChats, highInfoWords, folderToWrite='testfolder',train = 'BuildOnce')\n",
    "print('---> Done Topic Extraction. ' + str(k_best) + ' topics identified !!! ')\n",
    "print(transformed.count())\n",
    "endtime = datetime.now()\n",
    "timeElapsed = (endtime-starttime).total_seconds()\n",
    "print('Time Taken : ' + str(int(timeElapsed)/60.0) + ' min')\n",
    "\n",
    "### Trigram Extraction\n",
    "\n",
    "import os, shutil, string, re, json\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def remMiddleSpace(x):\n",
    "    strToken = x.split(' ')\n",
    "    tokLen = len(strToken)\n",
    "    if tokLen > 1:\n",
    "        return strToken[0]+strToken[1] # a short cut\n",
    "    else:\n",
    "        return x     \n",
    "    \n",
    "def remNone(val):\n",
    "    return unicode(val) or u''\n",
    "\n",
    "\n",
    "def get_ngrams(text, min = 1, max = 4):\n",
    "    s = []\n",
    "    for n in range(min, max):\n",
    "        for ngram in ngrams(text, n):\n",
    "            s.append('_'.join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n",
    "def fnReturnQueryString(quantileCuts,predictedTopic):\n",
    "    quantileCount = 0\n",
    "    queryStrings = []\n",
    "    for j in range(len(quantileCuts)+1):\n",
    "        if quantileCount == 0:\n",
    "            queryString = 'predictedTopic == {} and avgQCRscore <= {:.20f}'.format(predictedTopic,quantileCuts[0])\n",
    "            quantileCount += 1\n",
    "            queryStrings.append(queryString)\n",
    "        elif quantileCount == len(quantileCuts):\n",
    "            queryString = 'predictedTopic == {} and avgQCRscore >  {:.20f}'.format(predictedTopic,quantileCuts[len(quantileCuts)-1])\n",
    "            quantileCount += 1\n",
    "            queryStrings.append(queryString)\n",
    "        else:\n",
    "            queryString = 'predictedTopic == {} and avgQCRscore > {:.20f} and avgQCRscore <= {:.20f}'.format(predictedTopic,quantileCuts[quantileCount-1],quantileCuts[quantileCount])\n",
    "            quantileCount += 1\n",
    "            queryStrings.append(queryString)\n",
    "    return(queryStrings)\n",
    "\n",
    "def fnGetWordCounts(transformed,queryString):\n",
    "    '''Explanation of code:\n",
    "        1.retain chats for a predicted topic and quantile cut-off value\n",
    "        2.convert df to RDD\n",
    "        3.create a paired RDD\n",
    "        4.concatenate all chats into one giant string\n",
    "        5.find the top 50 trigrams\n",
    "        6.collect all trigrams into one list\n",
    "        7.collect all the words in the trigram into one list\n",
    "        8.create another key-value RDD of the type (word,1)\n",
    "        9.calculate word count\n",
    "        10.sort in descending order\n",
    "        11.show values\n",
    "       wordCounts = transformed.\\\n",
    "                        filter(queryString).\\\n",
    "                        select('chatID','sentence','chatcategory', 'avgQCRscore').\\\n",
    "                        rdd.\\\n",
    "                        map(lambda x: (x[1],x[2])).\\\n",
    "                        map(lambda x: (get_ngrams(str(x[0]).split(\" \"),2), x[1])).\\\n",
    "                        map(lambda x : (1,(' '.join(x[0]),x[1]))).\\\n",
    "                        aggregateByKey(zeroValue = ('', ''), seqFunc = (lambda x,y: (x[0] + ' ' + y[0], x[1] + ' ' + y[1])) , \\\n",
    "                                       combFunc = (lambda x,y: (x[0] + ' ' + y[0], x[1] + ' ' + y[1]))).\\\n",
    "                        collect()''' \n",
    "\n",
    "    # Only the words (tokens)\n",
    "    #wordCounts = transformed.\\\n",
    "    #                    filter(queryString).\\\n",
    "    #                    select('chatID','sentence','chatcategory', 'avgQCRscore').\\\n",
    "    #                    rdd.\\\n",
    "    #                    map(lambda x : (1,(remNone(x[1]),remMiddleSpace(remNone(x[2]))))).\\\n",
    "    #                    aggregateByKey(zeroValue = ('', ''), seqFunc = (lambda x,y: (x[0] + ' ' + y[0], x[1] + ' ' + y[1])) , \\\n",
    "    #                                   combFunc = (lambda x,y: (x[0] + ' ' + y[0], x[1] + ' ' + y[1]))).collect()\n",
    "                        \n",
    "    # The n-grams\n",
    "    wordCounts = transformed.\\\n",
    "                        filter(queryString).\\\n",
    "                        select('chatID','sentence','chatcategory', 'avgQCRscore').\\\n",
    "                        rdd.\\\n",
    "                        map(lambda x: (remNone(x[1]),remMiddleSpace(remNone(x[2])))).\\\n",
    "                        map(lambda x: (get_ngrams(str(x[0]).split(\" \")), x[1])).\\\n",
    "                        map(lambda x : (1,(' '.join(x[0]),x[1]))).\\\n",
    "                        aggregateByKey(zeroValue = ('', ''), seqFunc = (lambda x,y: (x[0] + ' ' + y[0], x[1] + ' ' + y[1])) , \\\n",
    "                                       combFunc = (lambda x,y: (x[0] + ' ' + y[0], x[1] + ' ' + y[1]))).collect()\n",
    "\n",
    "    # a tuple of word counts and category counts\n",
    "    wordCounts = Counter(wordCounts[0][1][0].split(' ')), Counter(wordCounts[0][1][1].split(' '))\n",
    "    return(wordCounts)\n",
    "\n",
    "def fnQuantileCuts(topic,transformed):\n",
    "    quantileCuts = transformed.\\\n",
    "                    filter('predictedTopic == {}'.format(topic)).\\\n",
    "                    approxQuantile(col='avgQCRscore',probabilities=[0.5],relativeError=0.01)\n",
    "    return(quantileCuts)\n",
    "\n",
    "# 'predictedTopic == {}' : filters predictedTopic == 0 , then 1 ...\n",
    "# Calculates the approximate quantiles of a numerical column of a DataFrame. It gives a numeric value, in our case it is \n",
    "# 50% percentile i.e. the median\n",
    "\n",
    "def fnExtractTrigrams(spark, k_best, folderToWrite, transformed, fileName):\n",
    "    # Reading the original data-\n",
    "    transformed.coalesce(1).write.csv(path=folderToWrite + 'outputData/',header = True)\n",
    "    # # Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
    "    \n",
    "    ### Creating Folders and Directories\n",
    "    print('---> Creating Folders and Directories')\n",
    "    os.chdir(folderToWrite) # change directory \n",
    "    \n",
    "    if os.path.isdir('topic_trigram'):\n",
    "        shutil.rmtree('topic_trigram') # deletes the old directory structure\n",
    "        os.mkdir('topic_trigram') # creates a directory\n",
    "        os.chdir(folderToWrite + 'topic_trigram/') # change directory\n",
    "    else:\n",
    "        os.mkdir('topic_trigram')\n",
    "        os.chdir(folderToWrite + 'topic_trigram/')\n",
    "        \n",
    "    for i in range(k_best): # looping over all the topics\n",
    "        if os.path.isdir('topic'+str(i)):\n",
    "            shutil.rmtree('topic'+str(i))\n",
    "            os.mkdir('topic'+str(i))\n",
    "            os.chdir(folderToWrite + 'topic_trigram/'+'topic'+str(i))\n",
    "            \n",
    "        else:\n",
    "            os.mkdir('topic'+str(i))\n",
    "            os.chdir(folderToWrite + 'topic_trigram/'+'topic'+str(i))\n",
    "            \n",
    "        quantileCuts  = fnQuantileCuts(i,transformed)\n",
    "        \n",
    "        for j in range(2):\n",
    "            with open('wordCounts_' + str(j) + '.json', 'w') as fp:\n",
    "                json.dump(fnGetWordCounts(transformed,fnReturnQueryString(quantileCuts, predictedTopic=i)[j]),fp)\n",
    "                \n",
    "        os.chdir(folderToWrite + 'topic_trigram/')\n",
    "        \n",
    "    print('---> Files written to '+ folderToWrite )\n",
    "    \n",
    "fnExtractTrigrams(spark, k_best, folderToWrite, transformed,fileName)\n",
    "endtime = datetime.now()\n",
    "timeElapsed = (endtime-starttime).total_seconds()\n",
    "print('Time Taken : ' + str(int(timeElapsed/60.0)) + ' mins')\n",
    "\n",
    "### Topic Distribution\n",
    "\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "for name in glob.glob('*/wordCounts_*.json'):\n",
    "    print name\n",
    "    \n",
    "def find_grams(x):\n",
    "    num_grams = len(x.split('_'))\n",
    "    if num_grams == 1:\n",
    "        return 1\n",
    "    elif num_grams == 2:\n",
    "        return 2\n",
    "    elif num_grams == 3:\n",
    "        return 3\n",
    "    else:\n",
    "        return 100\n",
    "\n",
    "fullData = pd.DataFrame()\n",
    "for name in glob.glob('*/wordCounts_*.json'):\n",
    "    with open(name) as json_data: \n",
    "        data = json.load(json_data)\n",
    "        data0 = data[0] # word counts\n",
    "        data1 = data[1] # category counts\n",
    "    \n",
    "    data0 = dict((k, v) for k, v in data0.items() if k) # # removing the null (key)\n",
    "    ### Unpacking the categories and adding it to the data frame    \n",
    "    data1 = dict((k, v) for k, v in data1.items() if k) # removing the null (key) from the category dict\n",
    "    sorted_token_dict = sorted(data1.items(), key=operator.itemgetter(1), reverse = True)[0:5] \n",
    "    # sorting dict by values and taking the top 5\n",
    "    name_hmap, values = zip(*sorted_token_dict) # unpacking the tuple\n",
    "    \n",
    "    data = pd.DataFrame.from_dict(data0, orient='index').reset_index()  # df of the word counts\n",
    "    \n",
    "    data.columns = ['ChatKeyWords','count'] \n",
    "    data['topic'] = str(name).split('/')[0]\n",
    "    data['Quantile'] = re.findall(r'\\d+', name)[1]\n",
    "\n",
    "    for i in range(len(name_hmap)):\n",
    "        us_cat = 'PageCategory_'+str(i+1)\n",
    "        data[us_cat] = name_hmap[i]\n",
    "\n",
    "    data = data.sort_values(by='count', ascending=False)\n",
    "    fullData = fullData.append(data)\n",
    "    \n",
    "fullData['n_grams'] = fullData['ChatKeyWords'].apply(lambda x: find_grams(x))\n",
    "\n",
    "#cc_map_schema = StructType([StructField(name = 'chatID', dataType=StringType()),\n",
    "#                              StructField(name = 'chatContent', dataType=StringType()),\n",
    "#                              StructField(name = 'chatcategory', dataType=StringType()),\n",
    "#                              StructField(name = 'ErrorCode', dataType=StringType()),])\n",
    "\n",
    "#cc_map_df = cc_map_df.toDF(schema=cc_map_schema)\n",
    "#cc_map_df = cc_map_df.select('chatID', 'chatContent')\n",
    "#chat_topic_df = cc_map_df.join(transformed, on='chatID', how='inner')\n",
    "#final_chat_df = chat_topic_df.select('chatID', 'chatcategory', 'avgQCRscore', 'predictedTopic')\n",
    "# final_chat_df.write.csv('Suvro_final_chat_score.csv')\n",
    "\n",
    "fullData.to_csv('Suvro_fullOutput_ngrams.csv',index=False)\n",
    "mostNegativeChats.toDF().toPandas().to_csv('Suvro_chat_final_score.csv')\n",
    "\n",
    "mostNegativeChats.toDF().toPandas().to_csv('Suvro_chat_final_score.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
