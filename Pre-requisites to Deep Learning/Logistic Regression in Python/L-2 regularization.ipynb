{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = 2\n",
    "\n",
    "X = np.random.randn(N, D)\n",
    "\n",
    "# Center the 1st 50 points at (-2, -2)\n",
    "X[:50, :] = X[:50, :] - 2*np.ones((50, D))\n",
    "\n",
    "# Center the last 50 points at (2, 2)\n",
    "X[50:, :] = X[50:, :] + 2*np.ones((50, D))\n",
    "\n",
    "# Labels : first 50 are 0 and last 50 are 1\n",
    "T = np.array([0]*50 + [1]*50)\n",
    "\n",
    "# Add a column of ones which is the bias term\n",
    "ones = np.ones((N, 1))\n",
    "Xb = np.concatenate((ones, X), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly initialize the weights\n",
    "w = np.random.randn(D + 1)\n",
    "\n",
    "# Calculate the model output-\n",
    "z = Xb.dot(w)\n",
    "\n",
    "# Defining the sigmoid-\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "Y = sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80228785,  0.82521011,  0.78593005,  0.7975942 ,  0.74615277,\n",
       "        0.79880715,  0.8048046 ,  0.81400267,  0.77364124,  0.92784619])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the Cross Entropy Error\n",
    "def cross_entropy(T, Y):\n",
    "    E = 0\n",
    "    for i in range(N):\n",
    "        if T[i] == 1:\n",
    "            E -= np.log(Y[i])\n",
    "        else:\n",
    "            E -= np.log(1 - Y[i])\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.644206873\n",
      "1.56241673687e-05\n",
      "4.51865668077e-05\n",
      "0.000118275578087\n",
      "0.000283000045228\n",
      "0.000624562302814\n",
      "0.00128151359433\n",
      "0.00246184891091\n",
      "0.00445450708055\n",
      "0.00763021839886\n"
     ]
    }
   ],
   "source": [
    "# Let's do the Gradient Descent 100 times\n",
    "learning_rate = 0.1\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        print(cross_entropy(T, Y))\n",
    "        \n",
    "    # Gradient descent weight update with regularization\n",
    "    w += learning_rate * (Xb.T.dot(T - Y) - 0.1*w)\n",
    "    \n",
    "    # recalculate Y\n",
    "    Y = sigmoid(Xb.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final w:  [-0.03623634  5.66075953  6.28959367]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final w: \", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So, we can see that the weights have come very close to [0, 4, 4] which we get from the Closeed form of solution.\n",
    "# The values of w are smaller now.\n",
    "# Since we assume that it is normally distributed around 0, we get the values of weights much closer to zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
