{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = 2\n",
    "X = np.random.randn(N, D)\n",
    "\n",
    "X[:50, :] = X[:50, :] - 2*np.ones((50, D))  # first 50 points to be centred at x = -2, y = -2 i.e. (-2, 2)\n",
    "X[50:, :] = X[50:, :] + 2*np.ones((50, D))  # the last 50 points to be centred ar (2, 2)\n",
    "\n",
    "T = np.array([0]*50 + [1]*50)  # 1st 50 set to 0 and next 50 set to 1\n",
    "\n",
    "ones = np.array([[1]*N]).T\n",
    "\n",
    "Xb = np.concatenate((ones, X), axis = 1) # axis = 1 by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.62826271, -2.19338468],\n",
       "       [ 1.        , -1.72176812, -1.54939791],\n",
       "       [ 1.        , -3.04954825, -0.99874205]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w = np.random.randn(D + 1)  # D features and 1 bias term\n",
    "\n",
    "# Calculate the model output\n",
    "z = Xb.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66621626,  0.54868782,  0.3327221 ,  0.47955688,  0.74602761,\n",
       "        0.42356914,  0.2713722 ,  0.37904611,  0.79092609,  0.38434824])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Entropy Error Function\n",
    "\n",
    "def cross_entropy(T, Y):  # T: Target (i.e. the True Class) and Y: Prediction (i.e. the output from the sigmoid func ...b/w 0 & 1)\n",
    "    E = 0  # cross entropy error function\n",
    "    for i in range(N):\n",
    "        if T[i] == 1:\n",
    "            E -= np.log(Y[i])\n",
    "        else:\n",
    "            E -= np.log(1 - Y[i])\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.827251683\n",
      "0.001622955019\n",
      "0.00161898818787\n",
      "0.00161504137614\n",
      "0.0016111144319\n",
      "0.00160720720479\n",
      "0.00160331954596\n",
      "0.00159945130805\n",
      "0.00159560234518\n",
      "0.00159177251293\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent. We will do 100 iterations\n",
    "learning_rate = 0.1\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:  # print cross entropy every 10 steps so that we see it is decreasing\n",
    "        print(cross_entropy(T, Y))\n",
    "    \n",
    "    # gradient descent (this case it is Gradient Ascent) weight update\n",
    "    # w = w - learning rate * { derivative of (negative of log likelihood)} which is equivalent to \n",
    "    # w = w + learning rate * { derivative of log likelihood)}\n",
    "    w += learning_rate * Xb.T.dot(T - Y)  # calculating the weight, the last term is the gradient dJ/dW of the Cross Entropy Error fnc.\n",
    "    \n",
    "    # recalculate Y\n",
    "    Y = sigmoid(Xb.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can see that the value of our error term decreases with each iteration of Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final w:  [  0.43916758  12.7402527   14.26925868]\n"
     ]
    }
   ],
   "source": [
    "# Let's see the optimal value of w\n",
    "print(\"Final w: \", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so, the 1st value is 0 same as the closed solution, but the other two values seem to be very large. We will address that shortly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
